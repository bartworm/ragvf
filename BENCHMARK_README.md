# ğŸ”¬ Benchmark Incremental - GuÃ­a Completa

## ğŸ“‹ Â¿QuÃ© es?

El benchmark incremental compara **5 configuraciones progresivas** del sistema RAG, evaluando el impacto de cada mejora:

```
1. Baseline (Hito 1)        â†’ Sistema bÃ¡sico con Chroma
   â†“ +Mejora
2. + Tablas                 â†’ ExtracciÃ³n avanzada de tablas
   â†“ +Mejora
3. + Two-Stage Retrieval    â†’ RecuperaciÃ³n en dos etapas
   â†“ +Mejora
4. + Reranking              â†’ Reordenamiento con cross-encoder
   â†“ +Mejora
5. + Preprocessing          â†’ Preprocesamiento de queries
```

## ğŸ¯ Â¿QuÃ© mide?

Para las **mismas 50 preguntas**, compara:

### MÃ©tricas de rendimiento:
- â±ï¸ **Latencia** (tiempo de respuesta en ms)
- ğŸ“Š **NÃºmero de fuentes** recuperadas
- ğŸ¯ **Score promedio** de relevancia

### MÃ©tricas de calidad:
- ğŸ“„ **Uso de tablas** (% de queries que usan tablas)
- ğŸ” **Tipos de fuentes** (texto vs tablas vs descriptores)
- âœ… **ActivaciÃ³n de features** (preprocessing, reranking)

## ğŸš€ Uso RÃ¡pido

### 1ï¸âƒ£ Ejecutar benchmark completo (50 preguntas, 5 configs):
```bash
python benchmark.py
```

### 2ï¸âƒ£ Ver resultados en consola:
```bash
python visualize_benchmark.py
```

### 3ï¸âƒ£ Generar reporte Markdown:
```bash
python visualize_benchmark.py --format md
```

### 4ï¸âƒ£ Generar reporte HTML:
```bash
python visualize_benchmark.py --format html
```

---

## ğŸ“– Uso Avanzado

### Ejecutar solo algunas configuraciones:

```bash
# Solo baseline
python benchmark.py --config 1_baseline

# Solo config final
python benchmark.py --config 5_full
```

### Probar con menos preguntas:

```bash
# Solo primeras 10 preguntas (testing rÃ¡pido)
python benchmark.py --questions 10

# Solo primeras 25 preguntas
python benchmark.py --questions 25
```

### Cambiar directorio de salida:

```bash
python benchmark.py --output results/mi_benchmark
```

---

## ğŸ“ Estructura de Resultados

DespuÃ©s de ejecutar el benchmark, se genera:

```
results/benchmark_5configs/
â”œâ”€â”€ 1_baseline_results.json        # Resultados config 1
â”œâ”€â”€ 2_tablas_results.json          # Resultados config 2
â”œâ”€â”€ 3_two_stage_results.json       # Resultados config 3
â”œâ”€â”€ 4_reranking_results.json       # Resultados config 4
â”œâ”€â”€ 5_full_results.json            # Resultados config 5
â”œâ”€â”€ summary.json                   # Resumen comparativo
â”œâ”€â”€ REPORT.md                      # Reporte Markdown (si se generÃ³)
â””â”€â”€ REPORT.html                    # Reporte HTML (si se generÃ³)
```

### Formato de `*_results.json`:

```json
{
  "config_name": "1_baseline",
  "timestamp": "2024-12-18T23:30:00",
  "metrics": {
    "config_name": "1_baseline",
    "total_questions": 50,
    "avg_latency_ms": 450.5,
    "avg_sources": 3.2,
    "avg_score": 0.70,
    "total_text_sources": 150,
    "total_table_sources": 10,
    "table_usage_rate": 0.20,
    "preprocessing_rate": 0.0,
    "reranking_rate": 0.0
  },
  "results": [
    {
      "config_name": "1_baseline",
      "question_id": 1,
      "question": "Â¿QuÃ© es el benzoato de sodio?",
      "answer": "[Respuesta generada]",
      "latency_ms": 445.2,
      "num_sources": 3,
      "source_types": {"text": 3},
      "avg_score": 0.70,
      "preprocessing_enabled": false,
      "reranking_enabled": false
    },
    ...
  ]
}
```

---

## ğŸ“Š Ejemplo de Salida

### Consola:

```
================================================================================
ğŸ“Š RESULTADOS DEL BENCHMARK INCREMENTAL
================================================================================
Fecha: 2024-12-18T23:30:00
Preguntas evaluadas: 50
================================================================================

ConfiguraciÃ³n         Latencia     Fuentes    Score      Tablas%
--------------------------------------------------------------------------------
1_baseline              450ms       3.2        0.70        20%
2_tablas                520ms       7.0        0.73        40%
3_two_stage             480ms       8.0        0.80        50%
4_reranking             650ms       7.0        0.85        45%
5_full                  680ms       8.0        0.90        50%

================================================================================
ğŸ“ˆ ANÃLISIS
================================================================================
âš¡ Mejor latencia: 1_baseline (450ms)
ğŸ¯ Mejor score:    5_full (0.90)
ğŸ“Š MÃ¡s tablas:     3_two_stage (50%)
================================================================================
```

---

## ğŸ”§ PersonalizaciÃ³n

### Modificar las configuraciones:

Edita `benchmark.py`, mÃ©todo `create_pipeline()`:

```python
def create_pipeline(self, config_name: str) -> RAGPipeline:
    if config_name == "1_baseline":
        # Personaliza aquÃ­
        pipeline = RAGPipeline(
            retriever=retriever,
            llm=llm,
            k_retrieval=10,  # Cambia parÃ¡metros
            k_final=5
        )
```

### AÃ±adir nuevas preguntas:

Edita `data/questions.json`:

```json
{
  "id": 51,
  "question": "Tu nueva pregunta aquÃ­",
  "category": "tecnica",
  "expected_topics": ["tema1", "tema2"],
  "difficulty": "media"
}
```

---

## ğŸ“ InterpretaciÃ³n de Resultados

### Latencia:
- **MÃ¡s baja** = MÃ¡s rÃ¡pido (mejor para producciÃ³n)
- **Trade-off**: Configuraciones avanzadas son mÃ¡s lentas pero mejores

### Score:
- **MÃ¡s alto** = Documentos mÃ¡s relevantes
- **Esperable**: Configs avanzadas (con reranking) tienen mejor score

### Uso de Tablas:
- **MÃ¡s alto** = Mejor aprovechamiento de informaciÃ³n estructurada
- **Importante**: Tablas contienen datos crÃ­ticos (concentraciones, pH, etc.)

### AnÃ¡lisis tÃ­pico:

```
Config 1 (Baseline):     RÃ¡pido pero score bajo
Config 2 (+Tablas):      MÃ¡s lento, mejor info
Config 3 (+Two-stage):   Balanceado, buen score
Config 4 (+Reranking):   Mejor score, mÃ¡s lento
Config 5 (+Full):        MÃ¡xima calidad, mÃ¡xima latencia
```

**RecomendaciÃ³n**: Config 4 o 5 para producciÃ³n si la latencia es aceptable.

---

## ğŸ› Troubleshooting

### Error: "No se encontrÃ³ summary.json"
```bash
# SoluciÃ³n: Ejecuta primero el benchmark
python benchmark.py
```

### Benchmark muy lento:
```bash
# SoluciÃ³n: Usa menos preguntas
python benchmark.py --questions 10
```

### Mock retriever devuelve datos simulados:
```
# Esto es esperado si no has indexado PDFs reales
# Para usar datos reales:
1. Indexa PDFs en Qdrant
2. Reemplaza MockRetriever con TwoStageRetriever real en benchmark.py
```

---

## ğŸ“ˆ PrÃ³ximos Pasos

1. **Ejecutar con datos reales**:
   - Indexar PDFs en Qdrant
   - Reemplazar MockRetriever

2. **AÃ±adir mÃ©tricas adicionales**:
   - BLEU score (comparaciÃ³n con respuestas gold)
   - ROUGE score
   - PrecisiÃ³n@K

3. **Automatizar**:
   - Ejecutar benchmark automÃ¡ticamente en CI/CD
   - Comparar con benchmarks anteriores

4. **Visualizaciones**:
   - GrÃ¡ficos de latencia vs score
   - DistribuciÃ³n de tipos de fuentes
   - AnÃ¡lisis por categorÃ­a de pregunta

---

## ğŸ’¡ Tips

- ğŸš€ **Testing rÃ¡pido**: `--questions 5` para iteraciones rÃ¡pidas
- ğŸ“Š **Comparar incrementos**: Ejecuta configs individualmente con `--config`
- ğŸ“ **MÃºltiples runs**: Usa `--output` para guardar en carpetas diferentes
- ğŸ¯ **Enfoque**: Filtra preguntas por categorÃ­a para anÃ¡lisis especÃ­fico

---

**Â¿Preguntas?** Revisa el cÃ³digo en `benchmark.py` o `visualize_benchmark.py`
